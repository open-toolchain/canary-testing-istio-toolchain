---
defaultBaseImageVersion: latest
properties:
- name: DEPLOYMENT_FILE
  value: ${DEPLOYMENT_FILE}
  type: text
- name: IBM_CLOUD_API_KEY
  value: ${API_KEY}
  type: secure
- name: APP_NAME
  value: ${CF_APP_NAME}
- name: COMMONS_HOSTED_REGION
  value: ${COMMONS_HOSTED_REGION}
  type: text
  default: "https://raw.githubusercontent.com/open-toolchain/commons/master"
stages:
- name: BUILD
  inputs:
  - type: git
    branch: ${GIT_BRANCH}
    service: ${GIT_REPO}
  triggers:
  - type: commit
  properties:
  - name: DOCKER_ROOT
    value: ${DOCKER_ROOT}
    type: text
  - name: DOCKER_FILE
    value: Dockerfile
    type: text  
  jobs:
  - name: Pre-build check
    type: builder
    build_type: cr
    artifact_dir: ''
    target:
      region_id: ${REGISTRY_REGION_ID}
      api_key: ${API_KEY}
    namespace: ${REGISTRY_NAMESPACE}
    image_name: ${CF_APP_NAME}
    script: |-
      #!/bin/bash
      # uncomment to debug the script
      # set -x
      # copy the script below into your app code repo (e.g. ./scripts/check_prebuild.sh) and 'source' it from your pipeline job
      #    source ./scripts/check_prebuild.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/check_prebuild.sh")

      # Lints Dockerfile and checks presence of registry namespace.
      source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/check_prebuild.sh")
  - name: Build Docker image
    type: builder
    build_type: cr
    artifact_dir: output
    target:
      region_id: ${REGISTRY_REGION_ID}
      api_key: ${API_KEY}
    namespace: ${REGISTRY_NAMESPACE}
    image_name: ${CF_APP_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      #set -x
      # copy the script below into your app code repo (e.g. ./scripts/build_image_buildkit.sh) and 'source' it from your pipeline job
      #    source ./scripts/build_image_buildkit.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/build_image_buildkit.sh")

      # This script does build a Docker image into IBM Container Service private image registry.
      # Minting image tag using format: BUILD_NUMBER-BRANCH-COMMIT_ID-TIMESTAMP
      # Also copies information into a build.properties file, so they can be reused later on by other scripts (e.g. image url, chart name, ...)
      source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/build_image_buildkit.sh")
      # check if doi is integrated in this toolchain
      if jq -e '.services[] | select(.service_id=="draservicebroker")' _toolchain.json; then
        ibmcloud login --apikey ${IBM_CLOUD_API_KEY} --no-region
        ibmcloud doi publishbuildrecord --branch ${GIT_BRANCH} --repositoryurl ${GIT_URL} --commitid ${GIT_COMMIT} \
          --buildnumber ${BUILD_NUMBER} --logicalappname ${IMAGE_NAME} --status pass
      fi
  - name: Tests
    type: tester
    script: |-
      #!/bin/bash
      # set -x
      if [ -f ./test/run-tests.sh ]; then
        source ./test/run-tests.sh
        IFS=';' read -ra locations <<< "$FILE_LOCATIONS"
        IFS=';' read -ra types <<< "$TEST_TYPES"
        if jq -e '.services[] | select(.service_id=="draservicebroker")' _toolchain.json; then
          ibmcloud login --apikey $IBM_CLOUD_API_KEY --no-region
          for i in "${!locations[@]}"
          do
            echo "$i ${locations[i]} ${types[i]}"
            ibmcloud doi publishtestrecord --logicalappname=${APP_NAME} --buildnumber=$BUILD_NUMBER --filelocation=${locations[i]} --type=${types[i]}
          done
        fi
      else
        echo "Test runner script not found: ./test/run-tests.sh"
      fi
- name: VALIDATE
  inputs:
  - type: job
    stage: BUILD
    job: Build Docker image
  triggers:
  - type: stage
  properties:
  - name: buildprops
    value: build.properties
    type: file
  jobs:
  - name: Vulnerability Advisor
    type: tester
    test_type: vulnerabilityadvisor
    use_image_from_build_input: true
    fail_stage: false
    target:
      region_id: ${REGISTRY_REGION_ID}
      api_key: ${API_KEY}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      # set -x
      # copy the script below into your app code repo (e.g. ./scripts/check_vulnerabilities.sh) and 'source' it from your pipeline job
      #    source ./scripts/check_vulnerabilities.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/check_vulnerabilities.sh")

      # Check for vulnerabilities of built image using Vulnerability Advisor
      SOURCE_BUILD_NUMBER=$BUILD_NUMBER
      source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/check_vulnerabilities.sh")
- name: DEPLOY
  inputs:
  - type: job
    stage: BUILD
    job: Build Docker image
  triggers:
  - type: stage
  properties:
  - name: buildprops
    value: build.properties
    type: file
  - name: CLUSTER_NAMESPACE
    value: ${PROD_CLUSTER_NAMESPACE}
    type: text    
  jobs:
  - name: Pre-deploy check
    type: deployer
    target:
      api_key: ${API_KEY}
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      #set -x
      # copy the script below into your app code repo (e.g. ./scripts/check_predeploy.sh) and 'source' it from your pipeline job
      #    source ./scripts/check_predeploy_kubectl.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/check_predeploy_kubectl.sh")

      # Checks the cluster is ready, has a namespace configured with access to the private
      # image registry (using an IBM Cloud API Key). It also configures Helm Tiller service to later perform a deploy with Helm.
      source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/check_predeploy_kubectl.sh")
  - name: Deploy to Kubernetes
    type: deployer
    target:
      api_key: ${API_KEY}
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      #set -x
      # copy the script below into your app code repo (e.g. ./scripts/deploy_kubectl.sh) and 'source' it from your pipeline job
      #    source ./scripts/deploy_kubectl.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/deploy_kubectl.sh")

      # Infer parallelel deployment configuration for canary app, using Git branch name
      echo -e "Updating $DEPLOYMENT_FILE to represent canary deployment: add label version, modify deployment name"
      CANARY_DEPLOYMENT_FILE=canary-${DEPLOYMENT_FILE} # prefix original deployment file name
      DEPLOYMENT_NAME=$( cat ${DEPLOYMENT_FILE} | yq r - -j | jq -r '. | select(.kind=="Deployment") | .metadata.name' ) # read deployment name
      # Substitute deployment name and version label (yaml>json>substitute>yaml)
      cat ${DEPLOYMENT_FILE} | yq write -d0 - "spec.template.metadata.labels.version" "canary" | yq write -d0 - "metadata.name" "${DEPLOYMENT_NAME}-canary" | yq write -d0 - "metadata.labels.version" "canary" > ${CANARY_DEPLOYMENT_FILE}
      DEPLOYMENT_FILE=${CANARY_DEPLOYMENT_FILE} # replace original deployment file
      cat ${DEPLOYMENT_FILE}
      USE_ISTIO_GATEWAY=true
      source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/deploy_kubectl.sh")
- name: CANARY 0% (EMERGENCY STOP)
  inputs:
  - type: job
    stage: BUILD
    job: Build Docker image
  triggers:
    - type: stage
      enabled: false
  properties:
  - name: buildprops
    value: build.properties
    type: file
  - name: CLUSTER_NAMESPACE
    value: ${PROD_CLUSTER_NAMESPACE}
    type: text
  - name: VIRTUAL_SERVICE_FILE
    value: virtualservice-test.yaml
    type: text    
  jobs:
  - name: Stop canary traffic
    type: deployer
    target:
      api_key: ${API_KEY}
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      # set -x
      # copy the script below into your app code repo (e.g. ./scripts/istio_virtualservice_stable.sh) and 'source' it from your pipeline job
      #    source ./scripts/istio_virtualservice_stable.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/istio_virtualservice_stable.sh")

      # Route all traffic to "stable" destination (using Istio)
      echo "Stop routing traffic to 'canary' destination"
      source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/istio_virtualservice_stable.sh")
- name: CANARY DARK
  inputs:
  - type: job
    stage: BUILD
    job: Build Docker image
  triggers:
    - type: stage
      enabled: false
  properties:
  - name: buildprops
    value: build.properties
    type: file
  - name: CLUSTER_NAMESPACE
    value: ${PROD_CLUSTER_NAMESPACE}
    type: text
  - name: VIRTUAL_SERVICE_FILE
    value: virtualservice-canary-dark.yaml
    type: text  
  jobs:
  - name: Dark launch (FF)
    type: deployer
    target:
      api_key: ${API_KEY}
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      # set -x
      # copy the script below into your app code repo (e.g. ./scripts/istio_virtualservice_canary_dark.sh) and 'source' it from your pipeline job
      #    source ./scripts/istio_virtualservice_canary_dark.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/istio_virtualservice_canary_dark.sh")

      # Only traffic matching a user-agent rule is targeting "canary" destination, the rest goes to "stable" destination (using Istio)
      source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/istio_virtualservice_canary_dark.sh")
  - name: Functional tests
    type: deployer
    target:
      api_key: ${API_KEY}
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      echo "Run some validation tests"
      echo "Provide link to Prometheus/Istio dashboard"
  - name: Chaos tests
    type: deployer
    target:
      api_key: ${API_KEY}
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      echo "Run some chaos tests"
- name: CANARY 50%
  inputs:
  - type: job
    stage: BUILD
    job: Build Docker image
  triggers:
    - type: stage
      enabled: false
  properties:
  - name: buildprops
    value: build.properties
    type: file
  - name: CLUSTER_NAMESPACE
    value: ${PROD_CLUSTER_NAMESPACE}
    type: text
  - name: VIRTUAL_SERVICE_FILE
    value: virtualservice-test.yaml
    type: text
  jobs:
  - name: Route 50% traffic
    type: deployer
    target:
      api_key: ${API_KEY}
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      # set -x
      # copy the script below into your app code repo (e.g. ./scripts/istio_virtualservice_canary_weight.sh) and 'source' it from your pipeline job
      #    source ./scripts/istio_virtualservice_canary_weight.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/istio_virtualservice_canary_weight.sh")

      # Route a fraction of traffic to "canary" destination (CANARY_WEIGHT), and rest to "stable" destination (using Istio)
      CANARY_WEIGHT=50
      source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/istio_virtualservice_canary_weight.sh")
- name: CANARY FINALIZE (AFTER MERGE)
  inputs:
  - type: job
    stage: BUILD
    job: Build Docker image
  triggers:
    - type: stage
      enabled: false
  properties:
  - name: buildprops
    value: build.properties
    type: file
  - name: CLUSTER_NAMESPACE
    value: ${PROD_CLUSTER_NAMESPACE}
    type: text
  - name: VIRTUAL_SERVICE_FILE
    value: virtualservice-test.yaml
    type: text    
  jobs:
  - name: Stop canary traffic
    type: deployer
    target:
      api_key: ${API_KEY}
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      # set -x
      # copy the script below into your app code repo (e.g. ./scripts/istio_virtualservice_stable.sh) and 'source' it from your pipeline job
      #    source ./scripts/istio_virtualservice_stable.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/istio_virtualservice_stable.sh")

      # Route all traffic to "stable" destination (using Istio)
      echo "Stop routing traffic to 'canary' destination"
      source <(curl -sSL "${COMMONS_HOSTED_REGION}/scripts/istio_virtualservice_stable.sh")
  - name: Discard canary deploy
    type: deployer
    target:
      region_id: ${PROD_REGION_ID}
      api_key: ${API_KEY}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      # Read deployment name from deployment manifest 
      DEPLOYMENT_NAME=$( cat ${DEPLOYMENT_FILE} | yq r - -j | jq -r '. | select(.kind=="Deployment") | .metadata.name' ) # read deployment name
      echo "Delete canary deployment"
      kubectl delete deployment ${DEPLOYMENT_NAME}-canary --namespace ${CLUSTER_NAMESPACE}
